# ğŸ­ FMCG Lakehouse ETL â€” Databricks End-to-End Data Engineering

> **End-to-end ETL pipeline** consolidating FMCG retail data from two post-acquisition companies into a unified **Medallion Lakehouse** using Databricks, PySpark, Delta Lake & AWS S3.


## ğŸ“Œ Business Problem

**AtlÃ¶n** (large FMCG retailer) acquired **Sports Bar** (startup). The COO needed unified analytics across both companies â€” but the data couldn't talk to each other:

| Company | Data Status | Issues |
|---|---|---|
| **AtlÃ¶n** | âœ… Clean & structured | Already in Gold layer |
| **Sports Bar** | âŒ Messy & inconsistent | Nulls, negatives, spelling errors, format mismatches, multiple sources |

**Goal:** Build a reliable ETL pipeline that cleans Sports Bar's data and consolidates both companies into a **single analytics layer** for unified reporting.

### âœ… Success Criteria (COO Requirements)
1. Aggregated analytics for both companies in **one reliable dashboard**
2. **Low learning curve** for new data hires
3. **Scalable, long-term** automated solution

---

## ğŸ—ï¸ Architecture â€” Medallion Lakehouse

```
Source Systems           AWS S3            Databricks
(CSVs, APIs,      â†’   (Data Lake)   â†’   Bronze â†’ Silver â†’ Gold â†’ Dashboard
 WhatsApp exports)                        Raw     Cleaned  Analytics  + Genie AI
```

### Layer Responsibilities

| Layer | Purpose | Write Mode |
|---|---|---|
| ğŸŸ¤ **Bronze** | Raw ingestion from S3. Untouched. Audit metadata added. | OVERWRITE (dims) / APPEND (facts) |
| âšª **Silver** | PySpark cleaning: types, nulls, spelling, surrogate keys | Transform & validate |
| ğŸŸ¡ **Gold** | Delta MERGE upserts. Single source of truth. BI-ready. | MERGE (dims) / APPEND (facts) |

---

## ğŸ—‚ï¸ Project Structure

```
fmcg-lakehouse-etl-databricks/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ 01_setup_catalog.ipynb          # Create FMCG catalog + schemas
â”‚   â”‚   â””â”€â”€ 02_s3_connection.ipynb          # IAM role-based S3 connection
â”‚   â”‚
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ 01_ingest_dimensions.ipynb      # Load dim CSVs from S3 â†’ Bronze
â”‚   â”‚   â””â”€â”€ 02_ingest_facts.ipynb           # Load fact CSVs from S3 â†’ Bronze
â”‚   â”‚
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ 01_customers_processing.ipynb   # Clean & transform dim_customers
â”‚   â”‚   â”œâ”€â”€ 02_products_processing.ipynb    # Clean, regex fix, variant extract
â”‚   â”‚   â””â”€â”€ 03_facts_processing.ipynb       # Type cast, filter, derive columns
â”‚   â”‚
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ 01_dim_upserts.ipynb            # Delta MERGE for all dimensions
â”‚   â”‚   â”œâ”€â”€ 02_full_load_fact.ipynb         # Historical 5-month batch load
â”‚   â”‚   â”œâ”€â”€ 03_incremental_load_fact.ipynb  # Daily incremental S3 â†’ Gold
â”‚   â”‚   â””â”€â”€ 04_fact_sales_view.ipynb        # Denormalized view for BI + Genie
â”‚   â”‚
â”‚   â””â”€â”€ orchestration/
â”‚       â””â”€â”€ pipeline_job_config.json        # Databricks Jobs configuration
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ data_model.png
â”‚   â””â”€â”€ dashboard_screenshot.png
â”‚
â”œâ”€â”€ data_model/
â”‚   â””â”€â”€ star_schema.md
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“Š Data Model â€” Star Schema

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_customers  â”‚
                    â”‚  customer_id PK â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dim_products â”‚    â”‚   fact_orders   â”‚    â”‚  dim_gross_price â”‚
â”‚ product_id PKâ”œâ”€â”€â”€â”€â”‚ date            â”œâ”€â”€â”€â”€â”‚  product_id FK   â”‚
â”‚ category     â”‚    â”‚ product_id FK   â”‚    â”‚  fiscal_year     â”‚
â”‚ division     â”‚    â”‚ customer_id FK  â”‚    â”‚  gross_price     â”‚
â”‚ variant      â”‚    â”‚ sold_quantity   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ source_company  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   fact_sales    â”‚  â† Denormalized Gold view
                    â”‚  (All dims +    â”‚     for Dashboard & Genie AI
                    â”‚   fact joined)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Key Technical Implementations

### 1. Surrogate Key Generation (SHA-256)
```python
from pyspark.sql.functions import sha2, concat_ws

# Prevents key collisions between AtlÃ¶n and Sports Bar IDs
df = df.withColumn(
    "customer_id",
    sha2(concat_ws("_", col("customer_code"), col("source_company")), 256)
)
```

### 2. Data Quality Fixes (Silver Layer)
```python
from pyspark.sql.functions import regexp_replace, when, col

# Fix spelling errors
df = df.withColumn("category",
    regexp_replace(col("category"), "(?i)beverege", "Beverage"))

# Remove invalid records
df = df.filter(col("sold_quantity") > 0)

# Handle nulls
df = df.withColumn("category",
    when(col("category").isNull(), "Unknown").otherwise(col("category")))
```

### 3. Gold Layer MERGE (Upsert)
```python
from delta.tables import DeltaTable

gold = DeltaTable.forName(spark, "fmcg.gold.dim_customers")
gold.alias("gold") \
    .merge(df_silver.alias("src"), "gold.customer_id = src.customer_id") \
    .whenMatchedUpdateAll() \
    .whenNotMatchedInsertAll() \
    .execute()
```

### 4. Incremental Load â€” Exactly Once
```python
import boto3

s3 = boto3.client("s3")
new_files = s3.list_objects_v2(Bucket="bucket", Prefix="new_data/")["Contents"]

for file in new_files:
    df = spark.read.option("header", True).csv(f"s3://bucket/{file['Key']}")
    df.write.format("delta").mode("append").saveAsTable("fmcg.bronze.fact_orders")
    # Archive to prevent reprocessing
    s3.copy_object(Bucket="bucket",
                   CopySource=f"bucket/{file['Key']}",
                   Key=file['Key'].replace("new_data/", "processed/"))
    s3.delete_object(Bucket="bucket", Key=file['Key'])
```

---

## ğŸ”„ Pipeline Modes

### Batch â€” Historical Load (5 Months)
- One-time backfill of Sports Bar historical sales data
- All CSVs processed through Bronze â†’ Silver â†’ Gold
- Fact table: **APPEND** mode to preserve all historical records

### Incremental â€” Daily Load (From Nov 30 onwards)
- New daily files land in `s3://bucket/new_data/`
- Archived to `s3://bucket/processed/` after ingestion (**exactly-once**)
- Scheduled via **Databricks Jobs** at 11 PM daily (`0 23 * * *`)

---

## ğŸ› ï¸ Tech Stack

| Tool | Role |
|---|---|
| **Databricks Free Edition** | Unified platform â€” notebooks, jobs, dashboard, Genie |
| **Apache Spark / PySpark** | Distributed data transformation |
| **Delta Lake** | ACID transactions, MERGE upsert, time travel |
| **AWS S3** | Raw data lake storage |
| **AWS IAM Roles** | Credential-free, least-privilege S3 access |
| **Python + boto3** | Pipeline scripting & S3 file operations |
| **SQL** | DDL, Gold view creation, validation |
| **Databricks Jobs** | Orchestration & cron scheduling |
| **Databricks Genie** | AI natural-language querying on Gold layer |
| **Databricks Dashboard** | AtlÃ¶n BI 360 â€” unified BI reporting |
| **SHA-256 Hashing** | Surrogate key generation |
| **Regex** | Spelling correction & variant extraction |

---

## ğŸ“ˆ Dashboard â€” AtlÃ¶n BI 360

Built on `fmcg.gold.fact_sales` denormalized view:

- ğŸ“Š **Counter KPIs** â€” Total Revenue, Total Quantity Sold
- ğŸ“… **Date & Category Filters** â€” Dynamic slicing
- ğŸ“Š **Bar Chart** â€” Top products by revenue
- ğŸ¥§ **Pie Chart** â€” Revenue share by sales channel
- ğŸ¤– **Genie AI** â€” Ask questions like *"Top 5 customers by revenue last quarter"*

---

## ğŸš€ How to Run

**Prerequisites:** Databricks Free Edition + AWS Free Tier account

```bash
# 1. Clone repo
git clone https://github.com/pawan-111/fmcg-lakehouse-etl-databricks.git

# 2. Import notebooks into Databricks workspace

# 3. Set up S3 bucket + IAM role (see notebooks/setup/)

# 4. Upload raw CSVs to S3 bucket

# 5. Run notebooks in order:
#    setup/ â†’ bronze/ â†’ silver/ â†’ gold/

# 6. Schedule Databricks Job using pipeline_job_config.json
```

---

## ğŸ”‘ Concepts Demonstrated

`Medallion Architecture` `Delta Lake MERGE` `Idempotent Pipeline` `Batch + Incremental Load` `SHA-256 Surrogate Keys` `Star Schema` `IAM Security` `Databricks Jobs` `Genie AI` `Denormalized Gold View` `PySpark Transformations` `Data Quality Engineering`

---

## ğŸ“š Reference

- ğŸ“¹ Full walkthrough: [YouTube Tutorial](https://youtu.be/U6ZUKWdfSLY)
- ğŸ§± [Databricks Free Edition](https://www.databricks.com/try-databricks)
- ğŸ“– [Delta Lake Docs](https://delta.io)

---

## ğŸ‘¤ Author

**Pawan** Â· Data Engineer Â· [GitHub @pawan-111](https://github.com/pawan-111)

> â­ Star this repo if it helped you!
